# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U8gBKZ_096Yu3J-kdYGTHmghNhWaLxLg
"""

#Haris Amaldi
 #Dilarang keras mengcopy kode ini! PLAGIARISME ITU HARAM!

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense,Bidirectional,Dropout

data_train = pd.read_csv('Indian Summers - Over the years.csv')
data_train.head()

len(data_train)

data_train.isnull().sum()

data_train.dropna(subset=['temp', 'feelslikemax', 'feelslikemin', 'feelslike', 'winddir', 'sealevelpressure'],inplace=True)
data_train.isnull().sum()

len(data_train)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
Tanggal = data_train['Date'].values
Temp = data_train['temp'].values
Tanggal_train, Tanggal_test, Temp_train, Temp_test = train_test_split(Tanggal, Temp, train_size=0.8, test_size=0.2, shuffle=False)
scaler = MinMaxScaler()
P_train = scaler.fit([Temp_train])
P_test = scaler.fit([Temp_test])

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(Temp_train, window_size=64, batch_size=200, shuffle_buffer=1000)
test_set = windowed_dataset(Temp_test, window_size=64, batch_size=200, shuffle_buffer=1000)


model = Sequential()
model.add(Bidirectional(LSTM(64, return_sequences=True))),
model.add(Bidirectional(LSTM(64))),
model.add(Dropout(0.5)),
model.add(Dense(64, activation="relu")),
model.add(Dense(32, activation="relu")),
model.add(Dense(1))

#mencari nilai MAE 10% skala data
Mae = (data_train['temp'].max() - data_train['temp'].min())*0.1
print(Mae)

#Hentikan jika MAE <10% skala data
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<2.06 and logs.get('val_mae')<2.06):
      print("Anda sudah berhasil menjalankan Proyek Time Series ini. MAE model ini sudah <10%. Selamat!")
      self.model.stop_training = True
callbacks = myCallback()

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set, epochs=100, validation_data = test_set, callbacks=[callbacks])

import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
plt.show()

import matplotlib.pyplot as plt
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Model MAE')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
plt.show()